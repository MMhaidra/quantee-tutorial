{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/logo_white_full.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperoptimization Tutorial\n",
    "\n",
    "In this part of the tutorial, we are going to learn what is **hyperoptimization**. While **parameters** are learned during training â€“ for example, the slope of linear regression or weights of neural network, **hyperparameters** are left for a data scientist to select beforehand.\n",
    "\n",
    "The selection of correct values for hyperparameters is crucial and can significantly improve the performance of a model. \n",
    "\n",
    "We could list three methods that are used for hyperoptimization:\n",
    "* **Grid Search** - most standard approach looking through whole hyperparameter space\n",
    "* **Random Search** - randomly select combinations from hyperparameter space\n",
    "* **Tree-structured Parzen Estimator (TPE)** - more intelligent way of tuning.\n",
    "\n",
    "As insurers and banks require interpretability, the most preferred and understandable method is Grid Search, however, you can play with other techniques too. To learn about Random Search and TPE, you can read [this post](http://dkopczyk.quantee.co.uk/hyperparameter-optimization/). \n",
    "\n",
    "The tutorial is based on [Allstate Claim Severity Kaggle competition data](https://www.kaggle.com/c/allstate-claims-severity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle # Load and save Python objects\n",
    "\n",
    "import numpy as np # Arrays\n",
    "import pandas as pd # Data-Frames\n",
    "from plotly.offline import init_notebook_mode # Plotly\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV # Hyperoptimization\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer # MAE\n",
    "from lightgbm import LGBMRegressor # Model to tune\n",
    "\n",
    "from utils import plot_gs_surface, plot_gs_scatter # Custom Utilities written for this tutorial\n",
    "\n",
    "import warnings # Ignore annoying warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Required for Jupyter to produce in-line Plotly graphs\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected estimator to tune is LightGBM. The plan is to:\n",
    "1. Download the training and testing **data** produced in the Data Processing Tutorial. Once again we do not use Kaggle test dataset as it is unlabelled.\n",
    "2. Select **hyperparameter space**.\n",
    "3. Perform **Grid Search** with 5-fold cross-validation.\n",
    "4. Check the **performance** on the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data\n",
    "Download the training and testing data that you have created during Data Processing Tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141238, 130)\n",
      "(47080, 130)\n"
     ]
    }
   ],
   "source": [
    "with open('data/data.pkl', 'rb') as f:\n",
    "    X_train, X_test, y_train, y_test = pickle.load(f)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Estimator\n",
    "Let's start with LightGBM estimator with default parameters. Just to remind you, we have transformed the losses to log-losses, so we need custom evaluation metric to maximize Kaggle's definition of MAE. From Regression Models Tutorial you might remember, that MAE of LightGBM with default parameters was 1152.14. Can we improve it with hyperoptimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_from_logs(y_true, y_pred):\n",
    "    return 'mae_from_logs', mean_absolute_error(np.exp(y_true), np.exp(y_pred)), False\n",
    "\n",
    "base = LGBMRegressor(n_jobs=-1, random_state=2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Hyperparameter space\n",
    "Grid Search is checking all combinations of hyperparameters' values and then returns the best set. For instance, if we want to optimize Neural Network hyperparameters: the number of layers $n_{layers}=[3,5,10]^T$ and number of neurons in each layer $n_{neurons}=[64,128]^T$ then Grid Search will fit the estimator 2x3=6 times on all possible combinations: $$(3,64), (3,128), (5,64), (5,128), (10,64), (10,128)$$\n",
    "\n",
    "To avoid **overfitting** problem, we should not hyperotimize on the same testing data as used for checking the overall performance. Thus, we can take the training data and split it again or better, use **cross-validation** and select the best set of hyperparameters based on k-fold score. Thus, we are going to use ```GridSearchCV``` implementation in scikit-learn.\n",
    "\n",
    "We are not going to optimize all LightGBM hyperparameters, but only important ones. Furthermore, the possible values may seem to be selected arbitrarily, but after some experience as a data scientist, you will see that there exist a common set of values to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_space = {'learning_rate': [0.001, 0.01, 0.1],\n",
    "               'n_estimators': [10, 50, 250],\n",
    "               'max_depth':  [4, 8, -1],\n",
    "               'num_leaves': [15, 31, 127],\n",
    "               'colsample_bytree': [0.6, 0.8, 1.0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we would run Grid Search CV on all possible combinations, but we would quickly notice that to produce 5-fold CV scores the number of fits would be 3x3x3x3x3x5=1215. That's a lot! You can try if you have a powerful CPU or GPU, but I propose to split the hyperparameter space into three parts:\n",
    "* Part 1: tune ```learning_rate``` and ```n_estimators```. They come together due to the fact that for gradient boosting methods there exists a trade-off between learning rate and a number of trees (the smaller the learning rate, the more trees we would need).\n",
    "* Part 2: tune ```max_depth``` and ```num_leaves```. The unconstrained depth of a tree can induce overfitting. Thus, when we try to tune a number of leaves we should control the maximum depth also.\n",
    "* Part 3: tune ```colsample_bytree```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_space_1 = {k: hyper_space[k] for k in ['learning_rate', 'n_estimators']}\n",
    "hyper_space_2 = {k: hyper_space[k] for k in ['max_depth', 'num_leaves']} \n",
    "hyper_space_3 = {k: hyper_space[k] for k in ['colsample_bytree']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Grid Search\n",
    "### Part 0: Preliminaries\n",
    "The ```GridSearchCV``` should select best set of hyperparameters based on cross-validated MAE calculated from log-losses. Thus, we define a custom scorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_from_logs_score(y_true, y_pred):\n",
    "    return mean_absolute_error(np.exp(y_true), np.exp(y_pred))\n",
    "\n",
    "mae_from_logs_scorer = make_scorer(mae_from_logs_score, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: ```learning_rate``` and ```n_estimators```\n",
    "Now, we tune the learning rate and number of estimators as well as the trade-off between them. Notice, that custom scorer for grid search is passed in ```GridSearchCV``` object initialization, whereas the fitting parameters for LGBMRegressor can be directly passed to ```fit``` method of ```GridSearchCV```. That all thanks to the scikit-learn wrapper of LightGBM!\n",
    "\n",
    "Now, run the following code snippet and wait a bit for the results. If you want more text logs to be output, change ```verbose``` argument to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'learning_rate': 0.1, 'n_estimators': 250}, -1155.1192355309925)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = GridSearchCV(base, hyper_space_1, scoring=mae_from_logs_scorer, cv=5, verbose=1)\n",
    "est.fit(X_train, y_train, eval_metric=mae_from_logs)\n",
    "est.best_params_, est.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine the results of Grid Search CV by accessing ```cv_results_``` property of fitted ```GridSearchCV```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.888909</td>\n",
       "      <td>0.090690</td>\n",
       "      <td>0.134342</td>\n",
       "      <td>0.007652</td>\n",
       "      <td>0.1</td>\n",
       "      <td>250</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 250}</td>\n",
       "      <td>-1151.240322</td>\n",
       "      <td>-1151.970104</td>\n",
       "      <td>-1178.674841</td>\n",
       "      <td>...</td>\n",
       "      <td>-1155.119236</td>\n",
       "      <td>12.560195</td>\n",
       "      <td>1</td>\n",
       "      <td>-1096.075996</td>\n",
       "      <td>-1097.017964</td>\n",
       "      <td>-1089.090743</td>\n",
       "      <td>-1095.453339</td>\n",
       "      <td>-1096.974588</td>\n",
       "      <td>-1094.922526</td>\n",
       "      <td>2.974013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.400648</td>\n",
       "      <td>0.008636</td>\n",
       "      <td>0.043737</td>\n",
       "      <td>0.011690</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 50}</td>\n",
       "      <td>-1194.121787</td>\n",
       "      <td>-1195.564874</td>\n",
       "      <td>-1215.262153</td>\n",
       "      <td>...</td>\n",
       "      <td>-1194.660514</td>\n",
       "      <td>12.667367</td>\n",
       "      <td>2</td>\n",
       "      <td>-1182.359816</td>\n",
       "      <td>-1182.966887</td>\n",
       "      <td>-1176.242977</td>\n",
       "      <td>-1181.476787</td>\n",
       "      <td>-1184.889720</td>\n",
       "      <td>-1181.587237</td>\n",
       "      <td>2.897586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.084647</td>\n",
       "      <td>0.264946</td>\n",
       "      <td>0.146839</td>\n",
       "      <td>0.012498</td>\n",
       "      <td>0.01</td>\n",
       "      <td>250</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 250}</td>\n",
       "      <td>-1276.972272</td>\n",
       "      <td>-1275.144329</td>\n",
       "      <td>-1293.758606</td>\n",
       "      <td>...</td>\n",
       "      <td>-1275.894746</td>\n",
       "      <td>11.158078</td>\n",
       "      <td>3</td>\n",
       "      <td>-1269.130215</td>\n",
       "      <td>-1269.835176</td>\n",
       "      <td>-1265.399640</td>\n",
       "      <td>-1268.201120</td>\n",
       "      <td>-1274.170738</td>\n",
       "      <td>-1269.347378</td>\n",
       "      <td>2.844238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.612357</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.018744</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 10}</td>\n",
       "      <td>-1441.946337</td>\n",
       "      <td>-1438.967286</td>\n",
       "      <td>-1460.766284</td>\n",
       "      <td>...</td>\n",
       "      <td>-1440.420907</td>\n",
       "      <td>11.457151</td>\n",
       "      <td>4</td>\n",
       "      <td>-1438.590882</td>\n",
       "      <td>-1438.795357</td>\n",
       "      <td>-1434.361492</td>\n",
       "      <td>-1437.921265</td>\n",
       "      <td>-1440.409883</td>\n",
       "      <td>-1438.015776</td>\n",
       "      <td>2.001648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.241092</td>\n",
       "      <td>0.022710</td>\n",
       "      <td>0.037489</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.01</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 50}</td>\n",
       "      <td>-1580.928700</td>\n",
       "      <td>-1580.153071</td>\n",
       "      <td>-1603.914835</td>\n",
       "      <td>...</td>\n",
       "      <td>-1582.701750</td>\n",
       "      <td>10.996244</td>\n",
       "      <td>5</td>\n",
       "      <td>-1582.084800</td>\n",
       "      <td>-1581.912153</td>\n",
       "      <td>-1577.107786</td>\n",
       "      <td>-1583.311355</td>\n",
       "      <td>-1584.572435</td>\n",
       "      <td>-1581.797706</td>\n",
       "      <td>2.533027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.085296</td>\n",
       "      <td>2.302225</td>\n",
       "      <td>0.143451</td>\n",
       "      <td>0.049516</td>\n",
       "      <td>0.001</td>\n",
       "      <td>250</td>\n",
       "      <td>{'learning_rate': 0.001, 'n_estimators': 250}</td>\n",
       "      <td>-1677.717755</td>\n",
       "      <td>-1677.793345</td>\n",
       "      <td>-1702.263059</td>\n",
       "      <td>...</td>\n",
       "      <td>-1680.557699</td>\n",
       "      <td>11.103780</td>\n",
       "      <td>6</td>\n",
       "      <td>-1680.825689</td>\n",
       "      <td>-1680.632047</td>\n",
       "      <td>-1675.155251</td>\n",
       "      <td>-1681.802900</td>\n",
       "      <td>-1682.150261</td>\n",
       "      <td>-1680.113230</td>\n",
       "      <td>2.544123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.574680</td>\n",
       "      <td>0.030371</td>\n",
       "      <td>0.021866</td>\n",
       "      <td>0.007654</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 10}</td>\n",
       "      <td>-1749.374195</td>\n",
       "      <td>-1750.318356</td>\n",
       "      <td>-1774.946446</td>\n",
       "      <td>...</td>\n",
       "      <td>-1753.106074</td>\n",
       "      <td>11.106976</td>\n",
       "      <td>7</td>\n",
       "      <td>-1753.817735</td>\n",
       "      <td>-1753.600206</td>\n",
       "      <td>-1747.669689</td>\n",
       "      <td>-1754.841036</td>\n",
       "      <td>-1754.736174</td>\n",
       "      <td>-1752.932968</td>\n",
       "      <td>2.676655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.666527</td>\n",
       "      <td>0.045028</td>\n",
       "      <td>0.054905</td>\n",
       "      <td>0.007293</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.001, 'n_estimators': 50}</td>\n",
       "      <td>-1776.645545</td>\n",
       "      <td>-1777.856832</td>\n",
       "      <td>-1802.489479</td>\n",
       "      <td>...</td>\n",
       "      <td>-1780.710130</td>\n",
       "      <td>11.049997</td>\n",
       "      <td>8</td>\n",
       "      <td>-1781.571243</td>\n",
       "      <td>-1781.322129</td>\n",
       "      <td>-1775.160959</td>\n",
       "      <td>-1782.618409</td>\n",
       "      <td>-1782.322121</td>\n",
       "      <td>-1780.598972</td>\n",
       "      <td>2.759986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.170227</td>\n",
       "      <td>0.061718</td>\n",
       "      <td>0.037089</td>\n",
       "      <td>0.010505</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_rate': 0.001, 'n_estimators': 10}</td>\n",
       "      <td>-1799.540957</td>\n",
       "      <td>-1800.974053</td>\n",
       "      <td>-1825.594592</td>\n",
       "      <td>...</td>\n",
       "      <td>-1803.854560</td>\n",
       "      <td>11.015323</td>\n",
       "      <td>9</td>\n",
       "      <td>-1804.836021</td>\n",
       "      <td>-1804.613700</td>\n",
       "      <td>-1798.281981</td>\n",
       "      <td>-1805.873329</td>\n",
       "      <td>-1805.460488</td>\n",
       "      <td>-1803.813104</td>\n",
       "      <td>2.801394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "8       3.888909      0.090690         0.134342        0.007652   \n",
       "7       1.400648      0.008636         0.043737        0.011690   \n",
       "5       5.084647      0.264946         0.146839        0.012498   \n",
       "6       0.612357      0.006248         0.018744        0.006248   \n",
       "4       1.241092      0.022710         0.037489        0.007653   \n",
       "2       7.085296      2.302225         0.143451        0.049516   \n",
       "3       0.574680      0.030371         0.021866        0.007654   \n",
       "1       2.666527      0.045028         0.054905        0.007293   \n",
       "0       1.170227      0.061718         0.037089        0.010505   \n",
       "\n",
       "  param_learning_rate param_n_estimators  \\\n",
       "8                 0.1                250   \n",
       "7                 0.1                 50   \n",
       "5                0.01                250   \n",
       "6                 0.1                 10   \n",
       "4                0.01                 50   \n",
       "2               0.001                250   \n",
       "3                0.01                 10   \n",
       "1               0.001                 50   \n",
       "0               0.001                 10   \n",
       "\n",
       "                                          params  split0_test_score  \\\n",
       "8    {'learning_rate': 0.1, 'n_estimators': 250}       -1151.240322   \n",
       "7     {'learning_rate': 0.1, 'n_estimators': 50}       -1194.121787   \n",
       "5   {'learning_rate': 0.01, 'n_estimators': 250}       -1276.972272   \n",
       "6     {'learning_rate': 0.1, 'n_estimators': 10}       -1441.946337   \n",
       "4    {'learning_rate': 0.01, 'n_estimators': 50}       -1580.928700   \n",
       "2  {'learning_rate': 0.001, 'n_estimators': 250}       -1677.717755   \n",
       "3    {'learning_rate': 0.01, 'n_estimators': 10}       -1749.374195   \n",
       "1   {'learning_rate': 0.001, 'n_estimators': 50}       -1776.645545   \n",
       "0   {'learning_rate': 0.001, 'n_estimators': 10}       -1799.540957   \n",
       "\n",
       "   split1_test_score  split2_test_score       ...         mean_test_score  \\\n",
       "8       -1151.970104       -1178.674841       ...            -1155.119236   \n",
       "7       -1195.564874       -1215.262153       ...            -1194.660514   \n",
       "5       -1275.144329       -1293.758606       ...            -1275.894746   \n",
       "6       -1438.967286       -1460.766284       ...            -1440.420907   \n",
       "4       -1580.153071       -1603.914835       ...            -1582.701750   \n",
       "2       -1677.793345       -1702.263059       ...            -1680.557699   \n",
       "3       -1750.318356       -1774.946446       ...            -1753.106074   \n",
       "1       -1777.856832       -1802.489479       ...            -1780.710130   \n",
       "0       -1800.974053       -1825.594592       ...            -1803.854560   \n",
       "\n",
       "   std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "8       12.560195                1        -1096.075996        -1097.017964   \n",
       "7       12.667367                2        -1182.359816        -1182.966887   \n",
       "5       11.158078                3        -1269.130215        -1269.835176   \n",
       "6       11.457151                4        -1438.590882        -1438.795357   \n",
       "4       10.996244                5        -1582.084800        -1581.912153   \n",
       "2       11.103780                6        -1680.825689        -1680.632047   \n",
       "3       11.106976                7        -1753.817735        -1753.600206   \n",
       "1       11.049997                8        -1781.571243        -1781.322129   \n",
       "0       11.015323                9        -1804.836021        -1804.613700   \n",
       "\n",
       "   split2_train_score  split3_train_score  split4_train_score  \\\n",
       "8        -1089.090743        -1095.453339        -1096.974588   \n",
       "7        -1176.242977        -1181.476787        -1184.889720   \n",
       "5        -1265.399640        -1268.201120        -1274.170738   \n",
       "6        -1434.361492        -1437.921265        -1440.409883   \n",
       "4        -1577.107786        -1583.311355        -1584.572435   \n",
       "2        -1675.155251        -1681.802900        -1682.150261   \n",
       "3        -1747.669689        -1754.841036        -1754.736174   \n",
       "1        -1775.160959        -1782.618409        -1782.322121   \n",
       "0        -1798.281981        -1805.873329        -1805.460488   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "8      -1094.922526         2.974013  \n",
       "7      -1181.587237         2.897586  \n",
       "5      -1269.347378         2.844238  \n",
       "6      -1438.015776         2.001648  \n",
       "4      -1581.797706         2.533027  \n",
       "2      -1680.113230         2.544123  \n",
       "3      -1752.932968         2.676655  \n",
       "1      -1780.598972         2.759986  \n",
       "0      -1803.813104         2.801394  \n",
       "\n",
       "[9 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = ['params','mean_test_score', 'std_test_score', 'mean_fit_time', 'mean_score_time']\n",
    "data_to_display = {k: est.cv_results_[k]  for k in keys}\n",
    "pd.DataFrame(data_to_display).sort_values(by='mean_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the 5-folded CV MAE. We will use Plotly package with predefined function that produces a surface plot. To check inner workings of the function, you can open it in the github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": true
       },
       "data": [
        {
         "colorscale": "Viridis",
         "reversescale": true,
         "showscale": false,
         "type": "surface",
         "uid": "e3f6210c-bbc3-4406-af9b-a0127672289d",
         "x": [
          0,
          1,
          2
         ],
         "y": [
          0,
          1,
          2
         ],
         "z": [
          [
           1803.8545601584415,
           1753.1060736590696,
           1440.420906574562
          ],
          [
           1780.7101297065676,
           1582.701750171143,
           1194.660514097081
          ],
          [
           1680.5576991134747,
           1275.8947457253792,
           1155.1192355309925
          ]
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "dragmode": "turntable",
        "margin": {
         "b": 10,
         "l": 10,
         "r": 10,
         "t": 0
        },
        "scene": {
         "xaxis": {
          "ticktext": [
           0.001,
           0.01,
           0.1
          ],
          "tickvals": [
           0,
           1,
           2
          ],
          "title": "learning_rate"
         },
         "yaxis": {
          "ticktext": [
           10,
           50,
           250
          ],
          "tickvals": [
           0,
           1,
           2
          ],
          "title": "n_estimators"
         },
         "zaxis": {
          "title": "5-fold CV MAE"
         }
        },
        "showlegend": false
       }
      },
      "text/html": [
       "<div id=\"5a08d8d0-6f4f-4468-b37a-03b14477b1c0\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"5a08d8d0-6f4f-4468-b37a-03b14477b1c0\", [{\"colorscale\": \"Viridis\", \"reversescale\": true, \"showscale\": false, \"x\": [0, 1, 2], \"y\": [0, 1, 2], \"z\": [[1803.8545601584415, 1753.1060736590696, 1440.420906574562], [1780.7101297065676, 1582.701750171143, 1194.660514097081], [1680.5576991134747, 1275.8947457253792, 1155.1192355309925]], \"type\": \"surface\", \"uid\": \"e3f6210c-bbc3-4406-af9b-a0127672289d\"}], {\"autosize\": false, \"dragmode\": \"turntable\", \"margin\": {\"b\": 10, \"l\": 10, \"r\": 10, \"t\": 0}, \"scene\": {\"xaxis\": {\"ticktext\": [0.001, 0.01, 0.1], \"tickvals\": [0, 1, 2], \"title\": \"learning_rate\"}, \"yaxis\": {\"ticktext\": [10, 50, 250], \"tickvals\": [0, 1, 2], \"title\": \"n_estimators\"}, \"zaxis\": {\"title\": \"5-fold CV MAE\"}}, \"showlegend\": false}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"5a08d8d0-6f4f-4468-b37a-03b14477b1c0\"));});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"5a08d8d0-6f4f-4468-b37a-03b14477b1c0\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"5a08d8d0-6f4f-4468-b37a-03b14477b1c0\", [{\"colorscale\": \"Viridis\", \"reversescale\": true, \"showscale\": false, \"x\": [0, 1, 2], \"y\": [0, 1, 2], \"z\": [[1803.8545601584415, 1753.1060736590696, 1440.420906574562], [1780.7101297065676, 1582.701750171143, 1194.660514097081], [1680.5576991134747, 1275.8947457253792, 1155.1192355309925]], \"type\": \"surface\", \"uid\": \"e3f6210c-bbc3-4406-af9b-a0127672289d\"}], {\"autosize\": false, \"dragmode\": \"turntable\", \"margin\": {\"b\": 10, \"l\": 10, \"r\": 10, \"t\": 0}, \"scene\": {\"xaxis\": {\"ticktext\": [0.001, 0.01, 0.1], \"tickvals\": [0, 1, 2], \"title\": \"learning_rate\"}, \"yaxis\": {\"ticktext\": [10, 50, 250], \"tickvals\": [0, 1, 2], \"title\": \"n_estimators\"}, \"zaxis\": {\"title\": \"5-fold CV MAE\"}}, \"showlegend\": false}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"5a08d8d0-6f4f-4468-b37a-03b14477b1c0\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_gs_surface(est, x_axis='learning_rate', y_axis='n_estimators',\n",
    "                z_axis='5-fold CV MAE', greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: ```max_depth``` and ```num_leaves```\n",
    "As, we already tuned number of estimators and learning rate, we can proceed to the next pair of hyperparameters. Let's quickly review them:\n",
    "- **max_depth**: it describes the maximum depth of tree and is used to handle overfitting (anytime you discover your testing score is much worse than training score, you can lower this parameter).\n",
    "- **num_leaves**: number of leaves per tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:  3.8min finished\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": true
       },
       "data": [
        {
         "colorscale": "Viridis",
         "reversescale": true,
         "showscale": false,
         "type": "surface",
         "uid": "a4df22b6-871e-416d-93fe-422dfde4e906",
         "x": [
          0,
          1,
          2
         ],
         "y": [
          0,
          1,
          2
         ],
         "z": [
          [
           1164.912413533764,
           1161.5001125403217,
           1161.8799238193114
          ],
          [
           1164.5051787410684,
           1153.4327073377308,
           1155.1192355309925
          ],
          [
           1164.5051787410684,
           1155.6681769431948,
           1157.283312322545
          ]
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "dragmode": "turntable",
        "margin": {
         "b": 10,
         "l": 10,
         "r": 10,
         "t": 0
        },
        "scene": {
         "xaxis": {
          "ticktext": [
           4,
           8,
           -1
          ],
          "tickvals": [
           0,
           1,
           2
          ],
          "title": "max_depth"
         },
         "yaxis": {
          "ticktext": [
           15,
           31,
           127
          ],
          "tickvals": [
           0,
           1,
           2
          ],
          "title": "num_leaves"
         },
         "zaxis": {
          "title": "5-fold CV MAE"
         }
        },
        "showlegend": false
       }
      },
      "text/html": [
       "<div id=\"463c30d3-eddf-4df3-ab4d-5f8e17406443\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"463c30d3-eddf-4df3-ab4d-5f8e17406443\", [{\"colorscale\": \"Viridis\", \"reversescale\": true, \"showscale\": false, \"x\": [0, 1, 2], \"y\": [0, 1, 2], \"z\": [[1164.912413533764, 1161.5001125403217, 1161.8799238193114], [1164.5051787410684, 1153.4327073377308, 1155.1192355309925], [1164.5051787410684, 1155.6681769431948, 1157.283312322545]], \"type\": \"surface\", \"uid\": \"a4df22b6-871e-416d-93fe-422dfde4e906\"}], {\"autosize\": false, \"dragmode\": \"turntable\", \"margin\": {\"b\": 10, \"l\": 10, \"r\": 10, \"t\": 0}, \"scene\": {\"xaxis\": {\"ticktext\": [4, 8, -1], \"tickvals\": [0, 1, 2], \"title\": \"max_depth\"}, \"yaxis\": {\"ticktext\": [15, 31, 127], \"tickvals\": [0, 1, 2], \"title\": \"num_leaves\"}, \"zaxis\": {\"title\": \"5-fold CV MAE\"}}, \"showlegend\": false}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"463c30d3-eddf-4df3-ab4d-5f8e17406443\"));});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"463c30d3-eddf-4df3-ab4d-5f8e17406443\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"463c30d3-eddf-4df3-ab4d-5f8e17406443\", [{\"colorscale\": \"Viridis\", \"reversescale\": true, \"showscale\": false, \"x\": [0, 1, 2], \"y\": [0, 1, 2], \"z\": [[1164.912413533764, 1161.5001125403217, 1161.8799238193114], [1164.5051787410684, 1153.4327073377308, 1155.1192355309925], [1164.5051787410684, 1155.6681769431948, 1157.283312322545]], \"type\": \"surface\", \"uid\": \"a4df22b6-871e-416d-93fe-422dfde4e906\"}], {\"autosize\": false, \"dragmode\": \"turntable\", \"margin\": {\"b\": 10, \"l\": 10, \"r\": 10, \"t\": 0}, \"scene\": {\"xaxis\": {\"ticktext\": [4, 8, -1], \"tickvals\": [0, 1, 2], \"title\": \"max_depth\"}, \"yaxis\": {\"ticktext\": [15, 31, 127], \"tickvals\": [0, 1, 2], \"title\": \"num_leaves\"}, \"zaxis\": {\"title\": \"5-fold CV MAE\"}}, \"showlegend\": false}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"463c30d3-eddf-4df3-ab4d-5f8e17406443\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Update base estimator with tuned learning_rate and n_estimators\n",
    "base.set_params(**est.best_params_)\n",
    "# Tune max_depth and num_leaves\n",
    "est = GridSearchCV(base, hyper_space_2, scoring=mae_from_logs_scorer, cv=5, verbose=1)\n",
    "est.fit(X_train, y_train, eval_metric=mae_from_logs)\n",
    "est.best_params_, est.best_score_\n",
    "# Plot\n",
    "plot_gs_surface(est, x_axis='max_depth', y_axis='num_leaves',\n",
    "                z_axis='5-fold CV MAE', greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: ```colsample_bytree```\n",
    "Lastly, we tune the feature fraction (aka colsample_bytree)\n",
    "- **colsample_bytree**: LightGBM will randomly select part of features on each iteration if feature_fraction smaller than 1.0. For example, if you set it to 0.8, LightGBM will select 80% of features before training each tree Tuning can be used to speed up training or to deal with overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": true
       },
       "data": [
        {
         "type": "scatter",
         "uid": "975d9e3e-99cf-45c4-aa22-6cfed8937646",
         "x": [
          0,
          1,
          2
         ],
         "y": [
          1154.0280333522528,
          1153.8972631201466,
          1153.4327073377308
         ]
        }
       ],
       "layout": {
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "xaxis": {
         "showgrid": false,
         "ticktext": [
          0.6,
          0.8,
          1
         ],
         "tickvals": [
          0,
          1,
          2
         ],
         "title": "colsample_bytree"
        },
        "yaxis": {
         "title": "5-fold CV MAE"
        }
       }
      },
      "text/html": [
       "<div id=\"0a5c0400-a3e6-448e-a405-8079ad0eea7a\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"0a5c0400-a3e6-448e-a405-8079ad0eea7a\", [{\"x\": [0, 1, 2], \"y\": [1154.0280333522528, 1153.8972631201466, 1153.4327073377308], \"type\": \"scatter\", \"uid\": \"975d9e3e-99cf-45c4-aa22-6cfed8937646\"}], {\"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"white\", \"xaxis\": {\"showgrid\": false, \"ticktext\": [0.6, 0.8, 1.0], \"tickvals\": [0, 1, 2], \"title\": \"colsample_bytree\"}, \"yaxis\": {\"title\": \"5-fold CV MAE\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"0a5c0400-a3e6-448e-a405-8079ad0eea7a\"));});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"0a5c0400-a3e6-448e-a405-8079ad0eea7a\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"0a5c0400-a3e6-448e-a405-8079ad0eea7a\", [{\"x\": [0, 1, 2], \"y\": [1154.0280333522528, 1153.8972631201466, 1153.4327073377308], \"type\": \"scatter\", \"uid\": \"975d9e3e-99cf-45c4-aa22-6cfed8937646\"}], {\"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"white\", \"xaxis\": {\"showgrid\": false, \"ticktext\": [0.6, 0.8, 1.0], \"tickvals\": [0, 1, 2], \"title\": \"colsample_bytree\"}, \"yaxis\": {\"title\": \"5-fold CV MAE\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"0a5c0400-a3e6-448e-a405-8079ad0eea7a\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Update base estimator with tuned max_depth and num_leaves\n",
    "base.set_params(**est.best_params_)\n",
    "# Tune max_depth and num_leaves\n",
    "est = GridSearchCV(base, hyper_space_3, scoring=mae_from_logs_scorer, cv=5, verbose=1)\n",
    "est.fit(X_train, y_train, eval_metric=mae_from_logs)\n",
    "est.best_params_, est.best_score_\n",
    "# Plot\n",
    "plot_gs_scatter(est, x_axis='colsample_bytree', y_axis='5-fold CV MAE', greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Performance\n",
    "Finally, we can use the best set of hyperparameters to check the performance of the model against testing data. The ```GridSearchCV``` object has argument ```refit=True``` as default, which means we can directly use ```est``` to predict testing labels and calculate the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE calculated on testing data = 1144.19\n"
     ]
    }
   ],
   "source": [
    "y_pred = est.predict(X_test)\n",
    "print('The MAE calculated on testing data = {0:.2f}'.format(mae_from_logs_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have started with MAE equal to 1152.14 and ended up with 1144.19. Well, it is not so much in that particular case, but we use only basics of hyperoptimization and it always depends to the use case. If you are experienced data scientist it is always worth to check whether tuning of hyperparameters can improve the predictability. \n",
    "\n",
    "---\n",
    "## Further notes\n",
    "* You can try to optimize other hyperparameters such as ```min_data_in_leaf```. For reference check LightGBM [docs](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html).\n",
    "* Change the ranges of hyperparameters. For instance ```colsample_bytree``` might be below 0.5 or try to increase ```n_estimators```.\n",
    "* Try out Random Search or TPE algorithms for hyperoptimization. They do not require so heavy calculations like Grid Search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
