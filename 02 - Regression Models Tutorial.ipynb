{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/logo_white_full.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Models Tutorial\n",
    "\n",
    "In this part of tutorial, we are going to explain **regression machine learning models** used for insurance data problem. The estimators considered:\n",
    "* **Linear Regression** (linear)\n",
    "* **Lasso** (linear)\n",
    "* **Support Vector Regression** (non-linear) \n",
    "* **Decision Tree** (non-linear) \n",
    "* **Random Forest** (non-linear, bagging)\n",
    "* **LightGBM Regressor** (non-linear, boosting)\n",
    "* **Neural Networks** (non-linear, deep learning)\n",
    "\n",
    "The tutorial is based on [Allstate Claim Severity Kaggle competition data](https://www.kaggle.com/c/allstate-claims-severity). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle # Load and save Python objects\n",
    "\n",
    "import numpy as np # Arrays\n",
    "import pandas as pd # Data-Frames\n",
    "from plotly.offline import init_notebook_mode # Plotly\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso # Linear models\n",
    "from sklearn.svm import SVR # SVR\n",
    "from sklearn.tree import DecisionTreeRegressor # Decision Tree\n",
    "from sklearn.ensemble import RandomForestRegressor # Random Forest\n",
    "from lightgbm import LGBMRegressor # LightGBM Regressor\n",
    "from keras.wrappers.scikit_learn import KerasRegressor # Keras Regressor\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures # Interactions for linear models\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer # MAE\n",
    "\n",
    "from utils import plot_nn_history # Custom Utilities written for this tutorial\n",
    "\n",
    "import warnings # Ignore annoying warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Required for Jupyter to produce in-line Plotly graphs\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases the best performance is reached by LightGBM or Deep Neural Networks (DNN), so why to bother with other estimators? Sometimes, it can turn out that linear models such as Linear Regression or Lasso are almost as good as more sophisticated estimators. Then, it might be more beneficial for the insurer or bank to use simpler models, as they are more interpretable (at least in regulator's point of view). \n",
    "\n",
    "It is a good point to make a note here - please remember, no matter what our clients say - it is a data scientist responsibility to teach banks and insurers the advantages of complex machine learning models. There are certain reasons why AI is so successful, and you with a little work you can also make _a black box_ more interpretable with clever visualizations. \n",
    "\n",
    "Ok, let's back to the tutorial. We are going to:\n",
    "1. Download the training and testing **data** produced in the Data Processing Tutorial. Once again, we do not use Kaggle test dataset as it is unlabelled.\n",
    "2. Show how to **train each estimator** and what is the best practice. \n",
    "3. Check the **performance** on the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data\n",
    "Download the training and testing data that we have created during Data Processing Tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141238, 130)\n",
      "(47080, 130)\n"
     ]
    }
   ],
   "source": [
    "with open('data/data.pkl', 'rb') as f:\n",
    "    X_train, X_test, y_train, y_test = pickle.load(f)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, let's download the names of columns, as we will use them for results analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat1</td>\n",
       "      <td>cat2</td>\n",
       "      <td>cat3</td>\n",
       "      <td>cat4</td>\n",
       "      <td>cat5</td>\n",
       "      <td>cat6</td>\n",
       "      <td>cat7</td>\n",
       "      <td>cat8</td>\n",
       "      <td>cat9</td>\n",
       "      <td>cat10</td>\n",
       "      <td>...</td>\n",
       "      <td>cont5</td>\n",
       "      <td>cont6</td>\n",
       "      <td>cont7</td>\n",
       "      <td>cont8</td>\n",
       "      <td>cont9</td>\n",
       "      <td>cont10</td>\n",
       "      <td>cont11</td>\n",
       "      <td>cont12</td>\n",
       "      <td>cont13</td>\n",
       "      <td>cont14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2     3     4     5     6     7     8      9     ...      120  \\\n",
       "0  cat1  cat2  cat3  cat4  cat5  cat6  cat7  cat8  cat9  cat10   ...    cont5   \n",
       "\n",
       "     121    122    123    124     125     126     127     128     129  \n",
       "0  cont6  cont7  cont8  cont9  cont10  cont11  cont12  cont13  cont14  \n",
       "\n",
       "[1 rows x 130 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/features.pkl', 'rb') as f:\n",
    "    features = pickle.load(f)\n",
    "pd.DataFrame(features).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## A note on MAE\n",
    "Mean Absolute Error (MAE) is the metric used for evaluation in this Kaggle competition. In Data Processing Tutorial, we log-transformed the losses to produce less skewed distribution. Since MAE should be calculated on untransformed losses, let's define a custom scorer that will be used for performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_from_logs_score(y_true, y_pred):\n",
    "    return mean_absolute_error(np.exp(y_true), np.exp(y_pred))\n",
    "\n",
    "mae_from_logs_scorer = make_scorer(mae_from_logs_score, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Linear Regression\n",
    "![lr](img/lr.png)\n",
    "Linear Regression (LR) is the simplest machine learning model known from statistics lessons. It is linear in the coefficients: $$y_i = \\alpha_0 + \\alpha_1 x_{1,i}+\\alpha_2 x_{1,i}+\\dots+\\alpha_P x_{P,i}$$ and the parameters $\\alpha_k, k=0,\\dots,P$ can be uniqually determined using [least squares method](https://en.wikipedia.org/wiki/Least_squares) (only if a number of observations $N<=P$).\n",
    "\n",
    "Let's fit the LR using [scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). As Python is an object-oriented programming language, each estimator is actually an object with its methods (```fit```, ```predict```) and properties (```coef_```, ```intercept_```). \n",
    "\n",
    "Firstly, we initialize estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to use ```fit``` method, to train our estimator on training data using least squares method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we determined the parameters of LR, we can access them using ```coef_``` and ```intercept_``` properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intercept</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.426105</td>\n",
       "      <td>-0.173112</td>\n",
       "      <td>0.171549</td>\n",
       "      <td>0.150794</td>\n",
       "      <td>-0.035385</td>\n",
       "      <td>0.046122</td>\n",
       "      <td>-0.178747</td>\n",
       "      <td>0.112539</td>\n",
       "      <td>0.043915</td>\n",
       "      <td>-0.007865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040195</td>\n",
       "      <td>-0.04259</td>\n",
       "      <td>0.523897</td>\n",
       "      <td>0.092302</td>\n",
       "      <td>0.182382</td>\n",
       "      <td>-0.028205</td>\n",
       "      <td>-0.072335</td>\n",
       "      <td>-0.155528</td>\n",
       "      <td>0.280503</td>\n",
       "      <td>0.237703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   intercept      cat1      cat2      cat3      cat4      cat5      cat6  \\\n",
       "0   6.426105 -0.173112  0.171549  0.150794 -0.035385  0.046122 -0.178747   \n",
       "\n",
       "       cat7      cat8      cat9    ...        cont5    cont6     cont7  \\\n",
       "0  0.112539  0.043915 -0.007865    ...     0.040195 -0.04259  0.523897   \n",
       "\n",
       "      cont8     cont9    cont10    cont11    cont12    cont13    cont14  \n",
       "0  0.092302  0.182382 -0.028205 -0.072335 -0.155528  0.280503  0.237703  \n",
       "\n",
       "[1 rows x 131 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.append(lr.intercept_, lr.coef_), index=['intercept']+features).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to use ```predict``` method on testing data and calculate the score. We can also calculate MAE on training data, but please remember that decision about model performance should be always made on scores calculated on testing data (or use cross-validation). \n",
    "\n",
    "For LR, you can also use old-fashioned statistical methods such as adjusted R^2, AIC or BIC calculated **on training data**, but testing errors already provide a better estimate of true model error, as well as use fewer assumptions about underlying model. Testing errors can be also used for all models such as Neural Networks or Random Forest, for which there is no definition of AIC or BIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE calculated on testing data = 1292.90\n",
      "The MAE calculated on training data = 1287.62\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "print('The MAE calculated on testing data = {0:.2f}'.format(mae_from_logs_score(y_test, y_pred)))\n",
    "y_pred_train = lr.predict(X_train)\n",
    "print('The MAE calculated on training data = {0:.2f}'.format(mae_from_logs_score(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of Linear Regression**\n",
    "* linear model,\n",
    "* parameters determined using least squares method,\n",
    "* for non-linear data, LR will be characterized by high bias, i.e. the model would not catch all complexities of the underlying data and produce poor scores for both training and testing data,\n",
    "* we can boost the performance of LR by adding interactions between variables, such as: $$\\{x_1x_2\\}, \\{x_1^3x_4\\}, \\{x_1x_2x_5\\}, \\dots$$ but the form and number of these interactions is very difficult to be determined,\n",
    "* visualization methods: residuals plots, Q-Q plots, the statistical significance of parameters, outliers plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lasso\n",
    "The lasso method is an extension of linear regression with the inclusion of the regularization term. Lasso automatically performs feature selection in the course of training and enhances the prediction accuracy of the statistical model. It partially solves the problem of selecting important interactions in LR, as it automatically chooses the right ones. \n",
    "\n",
    "The objective minimized in lasso has a form: $$F=\\sum_{i=1}^{N}\\left( y_i-\\alpha_0-\\sum_{k=1}^P \\alpha_j x_{ij}\\right)^2 + \\gamma \\sum_{k=1}^P |\\alpha_j|$$ which obviously cannot be optimized using least-squres method, thus algorithms like coordinate descent are used.\n",
    "\n",
    "Let's fit Lasso to our data using [scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html). In contrary to LR, in Lasso we have one hyperparameter ```alpha``` (λ in the objective function above) that needs to be tuned by data scientist. As hyperoptimization is explained in another tutorial, we limit ourselves to an arbitrarly selected value (you can change the value by hand if you wish and see the results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE calculated on testing data = 1292.62\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha=0.001, random_state=2019)\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred = lasso.predict(X_test)\n",
    "print('The MAE calculated on testing data = {0:.2f}'.format(mae_from_logs_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the MAE using Lasso is quite similar compared to LR. Why is that? Well, we forgot to add interactions to our features, so that we could not really take benefits of Lasso! We will add interactions up to degree 3 using [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) ```PolynomialFeatures``` only for some **categorical variables**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE calculated on testing data with interactions = 1246.95\n"
     ]
    }
   ],
   "source": [
    "# Indices of some categorical variables\n",
    "idx = [i for i, f in enumerate(features) if f.startswith('cat1')]\n",
    "\n",
    "# Create interactions for both training and testing data\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "X_train_poly = np.concatenate((X_train, poly.fit_transform(X_train[:,idx])), axis=1)\n",
    "X_test_poly = np.concatenate((X_test, poly.fit_transform(X_test[:,idx])), axis=1)\n",
    "print(X_train_poly.shape)\n",
    "print(X_test_poly.shape)\n",
    "\n",
    "# Fit and predict\n",
    "lasso = Lasso(alpha=0.001, random_state=2019)\n",
    "lasso.fit(X_train_poly, y_train)\n",
    "y_pred = lasso.predict(X_test_poly)\n",
    "print('The MAE calculated on testing data with interactions = {0:.2f}'.format(mae_from_logs_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it's quite better. You can play around with engineering various interactions, but to be honest, more sophisticated machine learning models can handle non-linear data better and faster. \n",
    "\n",
    "**Summary of Lasso**\n",
    "* linear model,\n",
    "* automatically selects important features,\n",
    "* bias-variance trade-off can be controlled by the shrinkage parameter ```alpha```. Increase in λ leads to decreased variance but increased bias, thus by proper selection of the shrinkage parameter value we can increase the accuracy of the model,\n",
    "* still linear in coefficient and in case of a lot of interactions we can run out of memory or make the model unstable,\n",
    "* visualization methods: residuals plots, Q-Q plots, the statistical significance of parameters, outliers plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Support Vector Regression\n",
    "SVR is the first non-linear algorithm that we are going to try out. It gains the non-linearity through a kernel function such as Radial Basis Function (RBF). For more info look at this [great post](https://medium.com/coinmonks/support-vector-regression-or-svr-8eb3acf6d0ff). \n",
    "\n",
    "[SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) has one main hyperparameter which is the type of kernel (RBF, linear, polynomial) and then the following hyperparameters are dependent on the type of kernel function. We will arbitrarily select hyperparameters: RBF kernel and then C, gamma and epsilon.\n",
    "\n",
    "**Don't bother to run this piece, of code - it is extremely slow.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svr = SVR(kernel='rbf', C=1, gamma='scale', epsilon=.1)\n",
    "#svr.fit(X_train, y_train)\n",
    "#y_pred = svr.predict(X_test)\n",
    "#print('The MAE calculated on testing data = {0:.2f}'.format(mae_from_logs_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of Support Vector Regression**\n",
    "* non-linear model,\n",
    "* very slow in comparison to other non-linear models,\n",
    "* on perceptual tasks (vision, speech and so on), they are massively outclassed by deep neural networks. On structured data, they are outperformed by gradient boosted machines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Decision Tree\n",
    "Mathematically speaking, decision tree algorithm relies on a stratification of feature space $X_1, X_2, \\dots, X_P$ into $K$ non-overlapping regions $R_1, R_2, \\dots, R_K$. Based on the training examples, we calculate mean values $y_{R_k}$ in each region $k \\in 1,2,\\dots,K$ and these become predictions for each new observation falling into a region $R_k$. The training of decision trees involves proper stratification of the feature space in the regions $R_1, R_2, \\dots, R_K$ with an aim to minimize a selected metric, like RSS for regression problems.\n",
    "\n",
    "You can build a great intuition of this algorithm, reading [this post](https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE calculated on testing data = 1733.64\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=2019)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "print('The MAE calculated on testing data = {0:.2f}'.format(mae_from_logs_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the MAE is much worse than linear models. Decision trees for bigger training sets are characterized by low bias and high variance, which results in an overfitting problem. To deal with that problem we can merge many decision trees to lower variance. Please look at the next model - Random Forest.\n",
    "\n",
    "**Summary of Decision Tree**\n",
    "* non-linear model,\n",
    "* easy to explain - it results in a set of rules,\n",
    "* low bias, but very high variance, resulting in poor predictions,\n",
    "* basis of powerful bagging and boosting algorithms,\n",
    "* visualization methods: residuals plots, tree methods in case feature space is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Random Forest\n",
    "![rf](img/rf.png)\n",
    "Random forests overcome this flaw by averaging many decision trees based on the different subsets of a training data. The averaged decision trees are called weak learners, whereas random forests estimator is a strong learner. The decrease of variance is achieved by combining uncorrelated weak learners, which is achieved by bootstrap aggregation (**bagging**) on decision trees and **random selection of features subset** in each tree split. To accomplish this task for each $b=1,2,\\dots,B$, where $B$ is the number of averaged decision trees, we perform: \n",
    "* Sample $n$ observations with replacement from the training set $X$ and $Y$. These become $X_b$ and $Y_b$.\n",
    "* Train decision tree $\\hat{f}_b$ using $X_b$ and $Y_b$. During learning, randomly select feature subset that will be used in each tree split. \n",
    "\n",
    "Once training is completed, predicted value for an unseen observation is calculated as **average of individual decision trees** predictions $$\\hat{f}=\\frac{1}{B}\\sum_{b=1}^B \\hat{f}_b$$ Averaging $B$ decision trees trained on $B$ bootstrapped training sets **reduces the varianc**e, greatly **increasing the accuracy and stability** of the statistical model in comparison to single decision trees. \n",
    "\n",
    "As each individual decision tree is a non-linear model by dividing the feature space into smaller sub-spaces, random forests have an advantage of capturing non-linear effects, in contrary to linear regression or lasso. Moreover, less data preprocessing is required, as random forests can be used without interaction terms, scaling or transforming data.\n",
    "\n",
    "Let's fit the estimator to our training data and see the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE calculated on testing data = 1209.29\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=2019)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print('The MAE calculated on testing data = {0:.2f}'.format(mae_from_logs_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes a while to train a model, but we greatly improved MAE in comparison to linear models. Furthermore, we can play around with hyperparameters to improve this score even more, but for now, let's see if we can reach improvement with better models.\n",
    "\n",
    "**Summary of Random Forest**\n",
    "* non-linear model,\n",
    "* reduction in overfitting: by averaging several trees, there is a significantly lower risk of overfitting,\n",
    "* less variance: By using multiple trees, you reduce the chance of stumbling across a classifier that doesn’t perform well because of the relationship between the train and test data,\n",
    "* visualization methods: residual plots, feature importance, examples of bagged trees decision rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## LightGBM Regressor\n",
    "GBM stands for Gradient Boosting Machines, which are greatly explained [here](https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab?gi=555cf4533233). LightGBM and XGBoost are excellent implementations of the GBM algorithm and provides a great improvement of accuracy and execution time comparing to scikit-learn AdaBoost. \n",
    "\n",
    "The LightGBM library comes with a great scikit-learn wrapper for LightGBM, so that we can still use familiar ```fit```, ```predict```, etc. methods! Let's fit the estimator with default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE calculated on testing data = 1152.14\n"
     ]
    }
   ],
   "source": [
    "gbm = LGBMRegressor(n_estimators=100, n_jobs=-1, random_state=2019)\n",
    "gbm.fit(X_train, y_train)\n",
    "y_pred = gbm.predict(X_test)\n",
    "print('The MAE calculated on testing data = {0:.2f}'.format(mae_from_logs_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, clearly it is much faster than random forest. It also provides us a good MAE improvement!. To be honest LightGBM or XGBoost are the algorithms that Kaggle's grandmasters always use in their solutions.\n",
    "\n",
    "**Summary of LightGBM**\n",
    "* non-linear model\n",
    "* boosting methods outperform random forest\n",
    "* much faster and less memory consumption than scikit-learn implementations of GBM\n",
    "* paralell mode enabled and GPU-ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Neural Networks\n",
    "![nn](img/nn.png)\n",
    "Last, but not least - neural networks.\n",
    "\n",
    "Artificial neural networks (ANN) are computing systems inspired by biological neural networks located in animal brains and are used to approximate functions that are unknown. The basic component of ANN is an artificial neuron and the way how the neurons are connected is called a network topology. There are various types of ANN including feedforward neural network, radial basis function network (RBF) or recurrent neural network (RNN) with the first being most commonly used in machine learning applications. The fully-connected feedforward neural network consists of several layers including an input layer, a number of hidden layers and one output layer. It is characterized by the fact that each neuron in a layer has only directed connections to all neurons of the next layer. To describe the fully-connected feedforward neural network we use standard notation. The $j$-th neuron in the $l$-th layer of a network takes a number of inputs, $a_k^{(l-1)}$, where $k=1,\\dots, n$ is an index of a neuron in the previous layer consisting of $n$ neurons. An input is then multiplied by a corresponding weight, $w_{jk}^{(l)}$ and an output $a_j^{(l)}$ is calculated by some nonlinear function $g(z_j^{(l)})$ of the weighted input\n",
    "$$z_j^{(l)}=\\sum_{k=1}^n w_{jk}^{(l)} a_k^{(l-1)}$$\n",
    "The function $g$ is known as _activation function_ and the most commonly used functions are Heaviside step function, sigmoid function and Rectified Linear Unit (RELU) function. The output functions in the last layer are restricted by the type of supervised machine learning algorithm. In classification problems, we are interested in outputs of range $[0,1]$, therefore sigmoid function can be used, whereas in regression problems the output function is set as $g(z_j^{(l)})=z_j^{(l)}$. \n",
    "\n",
    "To produce the desired output given an input, ANN are trained by a learning algorithm. The learning algorithm optimizes weights with an aim of minimizing a predefined cost function such as mean squared error (MSE) in regression problems. The weights of ANN can be optimized with gradient descent procedures using efficient backpropagation algorithm. Generally, in each iteration of the backpropagation algorithm a gradient descent procedure updates the weights in the direction of the greatest decrease of a cost function:\n",
    "$$\\delta w_{jk}^{(l)} = - \\eta \\frac{\\partial C}{\\partial w_{jk}^{(l)}}$$\n",
    "where $C$ is the cost function and $\\eta$ is an adjustable nonnegative constant, which controls the rate of learning.\n",
    "\n",
    "We use Keras, a great library that is \"like\" scikit-wrapper for Tensorflow. First, we need to **build an architecture** of our neural network. Here is the great function that returns us the Keras NN model with standard fully-conntected deep learning architecure. Please note that there are some extras:\n",
    "* **Batch normalization layer** - helps to avoid overfitting by normalizing the neurons outputs.\n",
    "* **Dropout layer** - helps to avoid overfitting by 'forgetting' neurons outputs at some rate.\n",
    "As it is the regression problem, the last (output) layer is just a linear function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "    \n",
    "def nn_architecture(n_features, n_layers=3, n_neurons=512, act='relu', drop=0.0,\n",
    "                    loss='mean_squared_error', optimizer='Adam'):\n",
    "    \n",
    "    # Input\n",
    "    x = Input(name='inputs', shape=(n_features, ), dtype='float32')\n",
    "    o = x\n",
    "    \n",
    "    # Network\n",
    "    for layer in range(n_layers):\n",
    "        o = Dense(n_neurons, activation=act, name='dense' + str(layer + 1))(o)\n",
    "        if drop > 0:\n",
    "            o = Dropout(drop, name='drop' + str(layer + 1))(o)\n",
    "        o = BatchNormalization(name='norm' + str(layer + 1))(o)\n",
    "    o = Dense(1, activation='linear', name='output')(o)\n",
    "    \n",
    "    # Compile and return\n",
    "    model = Model(inputs=x, outputs=o)\n",
    "    model.summary()\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we initilize the estimator, with default architecture (3 layers, 512 neurons in each layer, activation function is RELU). The loss optimized is MSE and the optimizer used is Adam. We use mini-batch stochastic gradient descent with batch size of 512 and 10 epochs (in that case 1 epoch means N/batch_size runs of backpropagation algorithm, where N is the size of training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 130)               0         \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 512)               67072     \n",
      "_________________________________________________________________\n",
      "norm1 (BatchNormalization)   (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "norm2 (BatchNormalization)   (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense3 (Dense)               (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "norm3 (BatchNormalization)   (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 599,041\n",
      "Trainable params: 595,969\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "141238/141238 [==============================] - 8s 53us/step - loss: 12.8740\n",
      "Epoch 2/10\n",
      "141238/141238 [==============================] - 6s 40us/step - loss: 0.3919\n",
      "Epoch 3/10\n",
      "141238/141238 [==============================] - 6s 39us/step - loss: 0.3649\n",
      "Epoch 4/10\n",
      "141238/141238 [==============================] - 6s 39us/step - loss: 0.3553\n",
      "Epoch 5/10\n",
      "141238/141238 [==============================] - 6s 42us/step - loss: 0.3490\n",
      "Epoch 6/10\n",
      "141238/141238 [==============================] - 6s 40us/step - loss: 0.3448\n",
      "Epoch 7/10\n",
      "141238/141238 [==============================] - 6s 40us/step - loss: 0.3410\n",
      "Epoch 8/10\n",
      "141238/141238 [==============================] - 6s 39us/step - loss: 0.3344\n",
      "Epoch 9/10\n",
      "141238/141238 [==============================] - 6s 39us/step - loss: 0.3335\n",
      "Epoch 10/10\n",
      "141238/141238 [==============================] - 6s 40us/step - loss: 0.3350\n"
     ]
    }
   ],
   "source": [
    "nn = KerasRegressor(build_fn=nn_architecture, epochs=10, batch_size=512, n_features=len(features))\n",
    "fit_history = nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the MAE loss monitored is calculated on log-losses, that is why it so small. You can define your custom objective in Keras, but it is more advanced material. Now, as we optimized the weights of neural network, we can check the MAE on testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE calculated on testing data = 1294.93\n"
     ]
    }
   ],
   "source": [
    "y_pred = nn.predict(X_test)\n",
    "print('The MAE calculated on testing data = {0:.2f}'.format(mae_from_logs_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the MAE is not so good as we can expect from such a great algorithm. My result is 1294.93, you can have different as Tensorflow randomly initilize weights (and you have no control over it). Learning of neural network is an art, and therfore we need to do more.\n",
    "\n",
    "Firstly, we need to watch out for overfitting. As we monitor only loss on the training data, we can set ```validation_split``` argument to non-zero, so that some % of training data will be hold just for calculation of validation errors.\n",
    "\n",
    "As gradient descent reach the local minimum it can miss it due to the large learning rate. Keras has an implementation of automatic learning rate schedule ```ReduceLROnPlateu```, which will decrease the learning rate by ```factor``` after the validation error has not been improved for the last ```patience``` epochs. \n",
    "\n",
    "Moreover, we will increase number of epochs to a large value and use ```EarlyStopping``` callback, so that the training will stop if validation error has not been improved for the last ```patience``` epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=4, verbose=1, mode='min'),\n",
    "             ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=0.01, mode='min')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 130)               0         \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 512)               67072     \n",
      "_________________________________________________________________\n",
      "norm1 (BatchNormalization)   (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "norm2 (BatchNormalization)   (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense3 (Dense)               (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "norm3 (BatchNormalization)   (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 599,041\n",
      "Trainable params: 595,969\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n",
      "Train on 127114 samples, validate on 14124 samples\n",
      "Epoch 1/90\n",
      "127114/127114 [==============================] - 8s 66us/step - loss: 14.4638 - val_loss: 0.7265\n",
      "Epoch 2/90\n",
      "127114/127114 [==============================] - 4s 35us/step - loss: 0.4049 - val_loss: 0.3837\n",
      "Epoch 3/90\n",
      "127114/127114 [==============================] - 4s 35us/step - loss: 0.3686 - val_loss: 0.4243\n",
      "Epoch 4/90\n",
      "127114/127114 [==============================] - 4s 35us/step - loss: 0.3637 - val_loss: 0.3615\n",
      "Epoch 5/90\n",
      "127114/127114 [==============================] - 4s 35us/step - loss: 0.3496 - val_loss: 0.3454\n",
      "Epoch 6/90\n",
      "127114/127114 [==============================] - 4s 35us/step - loss: 0.3513 - val_loss: 0.3583\n",
      "Epoch 7/90\n",
      "127114/127114 [==============================] - 4s 35us/step - loss: 0.3430 - val_loss: 0.3676\n",
      "Epoch 8/90\n",
      "127114/127114 [==============================] - 5s 38us/step - loss: 0.3378 - val_loss: 0.3814\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 9/90\n",
      "127114/127114 [==============================] - 10s 80us/step - loss: 0.3161 - val_loss: 0.3187\n",
      "Epoch 10/90\n",
      "127114/127114 [==============================] - 5s 37us/step - loss: 0.3134 - val_loss: 0.3180\n",
      "Epoch 11/90\n",
      "127114/127114 [==============================] - 4s 33us/step - loss: 0.3130 - val_loss: 0.3223\n",
      "Epoch 12/90\n",
      "127114/127114 [==============================] - 4s 34us/step - loss: 0.3130 - val_loss: 0.3226\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 13/90\n",
      "127114/127114 [==============================] - 4s 33us/step - loss: 0.3091 - val_loss: 0.3161\n",
      "Epoch 14/90\n",
      "127114/127114 [==============================] - 4s 34us/step - loss: 0.3084 - val_loss: 0.3158\n",
      "Epoch 15/90\n",
      "127114/127114 [==============================] - 4s 34us/step - loss: 0.3085 - val_loss: 0.3159\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 16/90\n",
      "127114/127114 [==============================] - 4s 33us/step - loss: 0.3081 - val_loss: 0.3160\n",
      "Epoch 17/90\n",
      "127114/127114 [==============================] - 4s 33us/step - loss: 0.3080 - val_loss: 0.3158\n",
      "Epoch 18/90\n",
      "127114/127114 [==============================] - 4s 33us/step - loss: 0.3080 - val_loss: 0.3159\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 00018: early stopping\n",
      "The MAE calculated on testing data = 1198.56\n"
     ]
    }
   ],
   "source": [
    "nn = KerasRegressor(build_fn=nn_architecture, epochs=90, batch_size=512, n_features=len(features),\n",
    "                    validation_split=0.1, callbacks=callbacks)\n",
    "fit_history = nn.fit(X_train, y_train)\n",
    "y_pred = nn.predict(X_test)\n",
    "print('The MAE calculated on testing data = {0:.2f}'.format(mae_from_logs_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is better. My MAE on testing data is 1198.56 so we are reaching the LightGBM result. As you can see the learning of deep neural networks is not so easy, but once you apply a correct method you can outperform LightGBM or XGBoost results. \n",
    "\n",
    "Additionally, we can plot learning history to see the convergence, analyse the bias-variance tradeoff and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": true
       },
       "data": [
        {
         "name": "Train loss",
         "type": "scatter",
         "uid": "1d13cd74-77b1-416f-9704-6e5b2ea337c2",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16
         ],
         "y": [
          0.368553222161208,
          0.3637383662712012,
          0.3495908710448353,
          0.35127064991806084,
          0.3429615838254738,
          0.3378224971484517,
          0.31614863514892705,
          0.3133841573583406,
          0.3130326654679326,
          0.31296538347069336,
          0.30913028965871203,
          0.308443783292488,
          0.3085462964814123,
          0.308070783383416,
          0.3079987627381407,
          0.3080042123026486
         ]
        },
        {
         "name": "Val loss",
         "type": "scatter",
         "uid": "b2923b9d-a180-41c6-848e-c54515efdc8a",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16
         ],
         "y": [
          0.42433120155429005,
          0.3614992410138659,
          0.34538794461035316,
          0.3583442789068036,
          0.3675934364963813,
          0.38138584341485654,
          0.3186505227833691,
          0.3179607837843645,
          0.3223313102300834,
          0.32259882146324326,
          0.31613804121862826,
          0.3157917174696078,
          0.3159181880093404,
          0.3159508874365035,
          0.3158415110986991,
          0.31589969725927525
         ]
        }
       ],
       "layout": {
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "xaxis": {
         "showgrid": false,
         "title": "Epochs"
        },
        "yaxis": {
         "title": "loss"
        }
       }
      },
      "text/html": [
       "<div id=\"e2c69f3c-17cd-449b-96ba-ba49435a887c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"e2c69f3c-17cd-449b-96ba-ba49435a887c\", [{\"name\": \"Train loss\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], \"y\": [0.368553222161208, 0.3637383662712012, 0.3495908710448353, 0.35127064991806084, 0.3429615838254738, 0.3378224971484517, 0.31614863514892705, 0.3133841573583406, 0.3130326654679326, 0.31296538347069336, 0.30913028965871203, 0.308443783292488, 0.3085462964814123, 0.308070783383416, 0.3079987627381407, 0.3080042123026486], \"type\": \"scatter\", \"uid\": \"1d13cd74-77b1-416f-9704-6e5b2ea337c2\"}, {\"name\": \"Val loss\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], \"y\": [0.42433120155429005, 0.3614992410138659, 0.34538794461035316, 0.3583442789068036, 0.3675934364963813, 0.38138584341485654, 0.3186505227833691, 0.3179607837843645, 0.3223313102300834, 0.32259882146324326, 0.31613804121862826, 0.3157917174696078, 0.3159181880093404, 0.3159508874365035, 0.3158415110986991, 0.31589969725927525], \"type\": \"scatter\", \"uid\": \"b2923b9d-a180-41c6-848e-c54515efdc8a\"}], {\"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"white\", \"xaxis\": {\"showgrid\": false, \"title\": \"Epochs\"}, \"yaxis\": {\"title\": \"loss\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"e2c69f3c-17cd-449b-96ba-ba49435a887c\"));});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"e2c69f3c-17cd-449b-96ba-ba49435a887c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"e2c69f3c-17cd-449b-96ba-ba49435a887c\", [{\"name\": \"Train loss\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], \"y\": [0.368553222161208, 0.3637383662712012, 0.3495908710448353, 0.35127064991806084, 0.3429615838254738, 0.3378224971484517, 0.31614863514892705, 0.3133841573583406, 0.3130326654679326, 0.31296538347069336, 0.30913028965871203, 0.308443783292488, 0.3085462964814123, 0.308070783383416, 0.3079987627381407, 0.3080042123026486], \"type\": \"scatter\", \"uid\": \"1d13cd74-77b1-416f-9704-6e5b2ea337c2\"}, {\"name\": \"Val loss\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], \"y\": [0.42433120155429005, 0.3614992410138659, 0.34538794461035316, 0.3583442789068036, 0.3675934364963813, 0.38138584341485654, 0.3186505227833691, 0.3179607837843645, 0.3223313102300834, 0.32259882146324326, 0.31613804121862826, 0.3157917174696078, 0.3159181880093404, 0.3159508874365035, 0.3158415110986991, 0.31589969725927525], \"type\": \"scatter\", \"uid\": \"b2923b9d-a180-41c6-848e-c54515efdc8a\"}], {\"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"white\", \"xaxis\": {\"showgrid\": false, \"title\": \"Epochs\"}, \"yaxis\": {\"title\": \"loss\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"e2c69f3c-17cd-449b-96ba-ba49435a887c\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_nn_history(fit_history.history, metric_name='loss', epoch_start=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Further notes\n",
    "* Try to build similar tutorial on classification dataset.\n",
    "* Add some visualiztion plots in Plotly such as residual plots, feature importance plots for tree methods, etc.\n",
    "* Check out next tutorials - hyperoptimization and stacking to improve the predictability.\n",
    "* Play around with neural networks - it is extremely interesting topic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
