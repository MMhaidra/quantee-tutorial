{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/logo_white_full.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Tutorial\n",
    "\n",
    "Stacking (or Stacked Generalization) is a method of combining different models with the aim to **increase predictability**. In the simplest form, it uses the set of base estimators and then stacks up their predictions, which are used as training data in another model.\n",
    "\n",
    "The method usually gives a little gain in performance, however, if for our clients **predictability** is crucial and 1% increase in accuracy can mean a huge competitive advantage - then stacking is great for them!\n",
    "\n",
    "This part of the tutorial is greatly based on the Dawid Kopczyk [post about stacking](http://dkopczyk.quantee.co.uk/stacking/). Please read it before continuing, as this notebook contains just implementation.\n",
    "\n",
    "The tutorial is based on [Allstate Claim Severity Kaggle competition data](https://www.kaggle.com/c/allstate-claims-severity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle # Load and save Python objects\n",
    "from copy import copy as make_copy\n",
    "\n",
    "import numpy as np # Arrays\n",
    "import pandas as pd # Data-Frames\n",
    "from plotly.offline import init_notebook_mode # Plotly\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso # Linear models\n",
    "from sklearn.ensemble import RandomForestRegressor # Random Forest\n",
    "from lightgbm import LGBMRegressor # LightGBM Regressor\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer # MAE\n",
    "\n",
    "import warnings # Ignore annoying warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Required for Jupyter to produce in-line Plotly graphs\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data\n",
    "Download the training and testing data that we have created during Data Processing Tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141238, 130)\n",
      "(47080, 130)\n"
     ]
    }
   ],
   "source": [
    "with open('data/data.pkl', 'rb') as f:\n",
    "    X_train, X_test, y_train, y_test = pickle.load(f)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## A note on MAE\n",
    "Mean Absolute Error (MAE) is the metric used for evaluation in this Kaggle competition. In Data Processing Tutorial, we log-transformed the losses to produce less skewed distribution. Since MAE should be calculated on untransformed losses, let's define a custom scorer that will be used for performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_from_logs_score(y_true, y_pred):\n",
    "    return mean_absolute_error(np.exp(y_true), np.exp(y_pred))\n",
    "\n",
    "mae_from_logs_scorer = make_scorer(mae_from_logs_score, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stacking\n",
    "Let’s define base estimators of our stacking procedure and stacking regressor itself. Lasso, Random Forest and LightGBM regressors are used in base level and Linear Regression as stacking estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_reg = [Lasso(alpha=0.001, random_state=2019), \n",
    "            RandomForestRegressor(random_state=2019),\n",
    "            LGBMRegressor(n_jobs=-1, random_state=2019)]\n",
    "stck_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check each model MAE separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE calculated on testing data for Lasso = 1292.62\n",
      "The MAE calculated on testing data for RandomForestRegressor = 1258.01\n",
      "The MAE calculated on testing data for LGBMRegressor = 1152.14\n"
     ]
    }
   ],
   "source": [
    "for reg in base_reg:\n",
    "    \n",
    "    # Fit model\n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = reg.predict(X_test)\n",
    "    \n",
    "    # Calculate MAE\n",
    "    print('The MAE calculated on testing data for {0} = {1:.2f}'.format(reg.__class__.__name__, \n",
    "                                                                        mae_from_logs_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is LightGBM with MAE=1152.14. Can we improve it with stacking? :)\n",
    "\n",
    "We will implement a function creating hold out predictions, which will be stacked together for each fold creating a column with meta feature for each estimator. We need to pass:\n",
    "* ```ref```  – an object representing base regressor,\n",
    "* ```X```, ```y```  – training data and target,\n",
    "* ```cv``` – sklearn Cross Validation object, for example KFold\n",
    "The recipe is simple – divide X and y into folds according to passed cv , for each hold out fold create predictions and save them to meta feature column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hold_out_predict(reg, X, y, cv):\n",
    "        \n",
    "    \"\"\"Performing cross validation hold out predictions for stacking\"\"\"\n",
    "    \n",
    "    # Initilize\n",
    "    meta_features = np.zeros(X.shape[0]) \n",
    "    n_splits = cv.get_n_splits(X, y)\n",
    "    \n",
    "    # Loop over folds\n",
    "    print(\"Starting hold out prediction with {} splits for {}.\".format(n_splits, reg.__class__.__name__))\n",
    "    for train_idx, hold_out_idx in cv.split(X, y): \n",
    "        \n",
    "        # Split data\n",
    "        X_train = X[train_idx]    \n",
    "        y_train = y[train_idx]\n",
    "        X_hold_out = X[hold_out_idx]\n",
    "\n",
    "        # Fit estimator to K-1 parts and predict on hold out part\n",
    "        est = make_copy(reg)\n",
    "        est.fit(X_train, y_train)\n",
    "        y_hold_out_pred = est.predict(X_hold_out)\n",
    "        \n",
    "        # Fill in meta features\n",
    "        meta_features[hold_out_idx] = y_hold_out_pred\n",
    "\n",
    "    return meta_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, use just defined function to create meta features from all base estimators. We will use 4-fold CV. Additionally, we retrain the model on full training data and create testing meta features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hold out prediction with 4 splits for Lasso.\n",
      "Starting hold out prediction with 4 splits for RandomForestRegressor.\n",
      "Starting hold out prediction with 4 splits for LGBMRegressor.\n"
     ]
    }
   ],
   "source": [
    "# Define 4-fold CV\n",
    "cv = KFold(n_splits=4, random_state=2019)\n",
    "\n",
    "# Loop over classifier to produce meta features\n",
    "meta_train = []\n",
    "for reg in base_reg:\n",
    "    \n",
    "    # Create hold out predictions for a classifier\n",
    "    meta_train_reg = hold_out_predict(reg, X_train, y_train, cv)\n",
    "    \n",
    "    # Gather meta training data\n",
    "    meta_train.append(meta_train_reg)\n",
    "    \n",
    "meta_train = np.array(meta_train).T\n",
    "\n",
    "meta_test = []\n",
    "for reg in base_reg:\n",
    "    \n",
    "    # Create hold out predictions for a classifier\n",
    "    reg.fit(X_train, y_train)\n",
    "    meta_test_reg = reg.predict(X_test)\n",
    "    \n",
    "    # Gather meta training data\n",
    "    meta_test.append(meta_test_reg)\n",
    "    \n",
    "meta_test = np.array(meta_test).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having ```meta_train```  and  ```meta_test```  we are ready to fit stacking regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE calculated on testing data for stacking LinearRegression = 1149.12\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "stck_reg.fit(meta_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = stck_reg.predict(meta_test)\n",
    "print('The MAE calculated on testing data for stacking {0} = {1:.2f}'.format(stck_reg.__class__.__name__, \n",
    "                                                                             mae_from_logs_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The MAE from stacking regressor is better than the single best base model!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
